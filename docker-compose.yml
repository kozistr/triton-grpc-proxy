services:
  triton-server:
    image: nvcr.io/nvidia/tritonserver:24.07-py3
    restart: always
    command: bash -c "LD_PRELOAD=/usr/lib/$(uname -m)-linux-gnu/libtcmalloc.so.4:${LD_PRELOAD} && pip install transformers tokenizers && tritonserver --model-repository=/models"
    volumes:
      - C:\Users\zero\Desktop\triton-grpc-proxy-rs/model_repository:/models
    ports:
      - 8000:8000
      - 8001:8001
      - 8002:8002
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 8g
    ulimits:
      memlock: -1
      stack: 67108864
    healthcheck:
      test: ["CMD", "curl", "http://triton-server:8000/v2/health/ready"]
      interval: 3s
      timeout: 5s
      retries: 5
      start_period: 60s
  proxy-server:
    build:
      context: ./
      dockerfile: Dockerfile
    depends_on:
      triton-server:
        condition: service_healthy
    image: triton-proxy
    restart: always
    ports:
      - 8080:8080
    environment:
      TRITON_SERVER_URL: "http://triton-server"
      TRITON_SERVER_PORT_GRPC: 8001
      SERVER_PORT: 8080
      MODEL_VERSION: "1"
      MODEL_NAME: "model"
      INPUT_NAME: "text"
      OUTPUT_NAME: "embedding"
      EMBEDDING_SIZE: 1024
