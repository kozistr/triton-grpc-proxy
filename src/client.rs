use std::convert::{TryFrom, TryInto};

use http::uri::InvalidUri;
use tonic::metadata::{AsciiMetadataValue, MetadataValue};
use tonic::service::interceptor::InterceptedService;
use tonic::service::Interceptor;
use tonic::transport::channel::ClientTlsConfig;
use tonic::transport::Channel;
use tonic::Status;

use super::inference;
use super::inference::grpc_inference_service_client::GrpcInferenceServiceClient;

/// Adds bearer token auth to [`Client`]
#[derive(Clone)]
pub struct AuthInterceptor {
    token: Option<AsciiMetadataValue>,
}

impl Interceptor for AuthInterceptor {
    fn call(&mut self, mut request: tonic::Request<()>) -> Result<tonic::Request<()>, Status> {
        if let Some(token) = self.token.as_ref() {
            request
                .metadata_mut()
                .insert("authorization", token.clone());
        }

        Ok(request)
    }
}

impl AuthInterceptor {
    fn create(access_token: Option<&str>) -> Result<Self, Error> {
        if let Some(access_token) = access_token {
            let fmt_token: String = format!("Bearer {}", access_token);
            let token = Some(MetadataValue::try_from(fmt_token)?);
            Ok(AuthInterceptor { token })
        } else {
            Ok(AuthInterceptor { token: None })
        }
    }
}

/// Triton Client
#[derive(Debug, Clone)]
pub struct Client {
    /// Raw grpc client interfaces automatically generated by tonic
    ///
    /// Should not necessary to use this interface directly in most cases
    pub inner: GrpcInferenceServiceClient<InterceptedService<Channel, AuthInterceptor>>,
}

#[derive(thiserror::Error, Debug)]
pub enum Error {
    #[error("a gRPC transport error has occurred: {0}")]
    TransportError(#[from] tonic::transport::Error),
    #[error("the client was provided and invalid URI: {0}")]
    InvalidUri(#[from] InvalidUri),
    #[error("invalid access token")]
    InvalidAccessToken(#[from] tonic::metadata::errors::InvalidMetadataValue),
    #[error("grpc call returned error status")]
    ResponseError(#[from] Status),
}

macro_rules! wrap_grpc_method {
    ($doc:literal, $name:ident, $req_type:ty, $resp_type:ty) => {
        #[doc=$doc]
        pub async fn $name(&self, req: $req_type) -> Result<$resp_type, Error> {
            let response = self.inner.clone().$name(tonic::Request::new(req)).await?;
            Ok(response.into_inner())
        }
    };
}

macro_rules! wrap_grpc_method_no_args {
    ($doc:literal, $name:ident, $req_type:ty, $resp_type:ty) => {
        #[doc=$doc]
        pub async fn $name(&self) -> Result<$resp_type, Error> {
            let req: $req_type = Default::default();
            let response = self.inner.clone().$name(tonic::Request::new(req)).await?;
            Ok(response.into_inner())
        }
    };
}

impl Client {
    wrap_grpc_method_no_args!(
        "Check liveness of the inference server.",
        server_live,
        inference::ServerLiveRequest,
        inference::ServerLiveResponse
    );

    wrap_grpc_method_no_args!(
        "Check readiness of the inference server.",
        server_ready,
        inference::ServerReadyRequest,
        inference::ServerReadyResponse
    );

    wrap_grpc_method!(
        "Check readiness of a model in the inference server.",
        model_ready,
        inference::ModelReadyRequest,
        inference::ModelReadyResponse
    );

    wrap_grpc_method_no_args!(
        "Get server metadata.",
        server_metadata,
        inference::ServerMetadataRequest,
        inference::ServerMetadataResponse
    );

    wrap_grpc_method!(
        "Get model metadata.",
        model_metadata,
        inference::ModelMetadataRequest,
        inference::ModelMetadataResponse
    );

    wrap_grpc_method!(
        "Perform inference using a specific model.",
        model_infer,
        inference::ModelInferRequest,
        inference::ModelInferResponse
    );

    wrap_grpc_method!(
        "Get model configuration.",
        model_config,
        inference::ModelConfigRequest,
        inference::ModelConfigResponse
    );

    wrap_grpc_method!(
        "Get the cumulative inference statistics for a model.",
        model_statistics,
        inference::ModelStatisticsRequest,
        inference::ModelStatisticsResponse
    );

    wrap_grpc_method!(
        "Get the index of model repository contents.",
        repository_index,
        inference::RepositoryIndexRequest,
        inference::RepositoryIndexResponse
    );

    wrap_grpc_method!(
        "Load or reload a model from a repository.",
        repository_model_load,
        inference::RepositoryModelLoadRequest,
        inference::RepositoryModelLoadResponse
    );

    wrap_grpc_method!(
        "Unload a model.",
        repository_model_unload,
        inference::RepositoryModelUnloadRequest,
        inference::RepositoryModelUnloadResponse
    );

    wrap_grpc_method!(
        "Get the status of all registered system-shared-memory regions.",
        system_shared_memory_status,
        inference::SystemSharedMemoryStatusRequest,
        inference::SystemSharedMemoryStatusResponse
    );

    wrap_grpc_method!(
        "Register a system-shared-memory region.",
        system_shared_memory_register,
        inference::SystemSharedMemoryRegisterRequest,
        inference::SystemSharedMemoryRegisterResponse
    );

    wrap_grpc_method!(
        "Unregister a system-shared-memory region.",
        system_shared_memory_unregister,
        inference::SystemSharedMemoryUnregisterRequest,
        inference::SystemSharedMemoryUnregisterResponse
    );

    wrap_grpc_method!(
        "Get the status of all registered CUDA-shared-memory regions.",
        cuda_shared_memory_status,
        inference::CudaSharedMemoryStatusRequest,
        inference::CudaSharedMemoryStatusResponse
    );

    wrap_grpc_method!(
        "Register a CUDA-shared-memory region.",
        cuda_shared_memory_register,
        inference::CudaSharedMemoryRegisterRequest,
        inference::CudaSharedMemoryRegisterResponse
    );

    wrap_grpc_method!(
        "Unregister a CUDA-shared-memory region.",
        cuda_shared_memory_unregister,
        inference::CudaSharedMemoryUnregisterRequest,
        inference::CudaSharedMemoryUnregisterResponse
    );

    wrap_grpc_method!(
        "Update and get the trace setting of the Triton server.",
        trace_setting,
        inference::TraceSettingRequest,
        inference::TraceSettingResponse
    );

    /// Create a new triton client for the given url.
    pub async fn new(
        url: impl TryInto<http::Uri, Error = InvalidUri>,
        access_token: Option<String>,
    ) -> Result<Self, Error> {
        let mut channel = Channel::builder(url.try_into()?);

        if access_token.is_some() {
            channel = channel.tls_config(ClientTlsConfig::new())?;
        }

        let channel = channel.connect().await?;

        let client = GrpcInferenceServiceClient::with_interceptor(
            channel,
            AuthInterceptor::create(access_token.as_deref())?,
        );

        Ok(Client { inner: client })
    }
}
